================================================================================
‚ö° QUICK START GUIDE FOR AI - RESEARCH WORKFLOW
================================================================================

üö® **CRITICAL MANDATE: FRESH DATA FROM ALL SOURCES EVERY EXECUTION**
================================================================================
This workflow requires FRESH DATA from ALL sources, EVERY SINGLE RUN, WITHOUT EXCEPTION.

**MANDATORY DATA SOURCES (all required, every time):**
- YouTube transcripts (8 channels) - MANDATORY
- RSS feeds (5+ providers) - MANDATORY
- X/Twitter (3 lists + bookmarks) - MANDATORY
- Technical data (SPY/QQQ options) - MANDATORY
- Web searches (market data) - MANDATORY

**NO OPTIONAL DATA SOURCES. NO SKIPPING. NO WORKAROUNDS.**

If any scraper FAILS: RETRY ONCE, then STOP and report incomplete collection.
If data is EMPTY (zero results): That is VALID data - continue with empty summary.
If data is MISSING (scraper crashed): STOP - do NOT proceed without it.

**CACHING STRATEGY:** System may intelligently cache old data to avoid re-downloading unchanged sources, but PRIMARY OBJECTIVE is FRESH data input in real-time as possible.

================================================================================
üî¥ **DATA QUALITY & FAILURE REPORTING - CRITICAL IMPERATIVE**
================================================================================

**CONTEXT: LIFE OR DEATH DATA SYSTEM**
This research data directly feeds investment decisions. These are SERIOUS decisions made on this data.
Data quality, completeness, and timeliness are EXISTENTIAL to the system's viability.

**ANY DATA COLLECTION FAILURE MUST BE REPORTED IMMEDIATELY:**
- Scraper crashes or errors ‚Üí REPORT immediately (do not hide, do not skip)
- Data sources returning empty unexpectedly ‚Üí REPORT (may indicate source change/blocking)
- Network/connectivity issues ‚Üí REPORT
- API rate limits or authentication failures ‚Üí REPORT
- Missing data files that should exist ‚Üí REPORT

**FAILURE ESCALATION PROTOCOL:**
1. If scraper FAILS on first run: Retry ONCE with detailed logging
2. If STILL FAILING after retry: STOP and report with:
   - [ ] Which scraper failed (YouTube/RSS/X/Technical/Web searches)
   - [ ] Error message/exception details
   - [ ] Time of failure
   - [ ] Previous successful run timestamp
   - [ ] Action taken (retry? debug logs?)
3. DO NOT PROCEED with incomplete data - report to user immediately
4. DO NOT ATTEMPT WORKAROUNDS - workarounds hide real problems

**WHY THIS MATTERS:**
- Bad data ‚Üí Bad signals ‚Üí Bad trading decisions ‚Üí Real financial loss
- Incomplete data set ‚Üí Biased analysis ‚Üí Systematic blindspot in decision-making
- Silent failures ‚Üí Compounding errors over time
- You are the FIRST LINE OF DEFENSE for data quality

**REPORTING TEMPLATE (use at end of workflow):**
```
DATA COLLECTION SUMMARY
- YouTube: [‚úÖ Complete / ‚ùå Failed - details]
- RSS: [‚úÖ Complete / ‚ùå Failed - details]
- X/Twitter: [‚úÖ Complete / ‚ùå Failed - details]
- Technical Data: [‚úÖ Complete / ‚ùå Failed - details]
- Web Searches: [‚úÖ Complete / ‚ùå Failed - details]

Total fresh data files created: [X]
Status: [‚úÖ COMPLETE & VALID / ‚ùå INCOMPLETE - FAILED]
```

================================================================================

**IMPORTANT: Today's date for all file operations: YYYY-MM-DD**
Replace all instances of `YYYY-MM-DD` in this document with today's actual date (e.g., 2025-10-19).

**For AI agents running this workflow:**

1. **Use TodoWrite tool** to track all 5 steps and mark each completed
2. **STEP 0A: Verify scrapers have completed** - User will run scrapers manually, ASK USER to confirm completion
3. **STEP 0B: Run web searches in parallel** - Execute 8 searches simultaneously (2-3 min)
4. **STEP 0C: Verify all data is present** - Check that scraper outputs + web search data exist before Step 1
5. **Create 7 technical summaries early** - These are blocking requirements
6. **Use parallel tool calls** when independent - maximize speed (see Step 1.3 parallel optimization)
7. **Review signal output** - If trend score = 0 but market shows support, make AI adjustments
8. **MANDATORY: ALL DATA SOURCES EVERY TIME**
   - YouTube transcripts: MANDATORY - run scraper EVERY execution
   - RSS feeds: MANDATORY - run scraper EVERY execution
   - X/Twitter: MANDATORY - run scraper EVERY execution (including bookmarks)
   - Technical data: MANDATORY - run scraper EVERY execution
   - Web searches: MANDATORY - run searches EVERY execution
   - **NO EXCEPTIONS:** Fresh data from ALL sources, EVERY RUN, WITHOUT FAIL
   - **CACHING STRATEGY:** System may cache old data to avoid re-downloading unchanged sources, but primary goal is FRESH data input in real-time as possible
8. **Always report completion** - Use the full report format with all 16 files verified

**CRITICAL WORKFLOW SAFEGUARDS (Prevent Blocking Failures):**

**FIX #1: STEP 1 VERIFICATION CHECKPOINT** (Add after Step 1.5 completes)
- BEFORE proceeding to Step 2, verify ALL MANDATORY summaries exist using Glob patterns
- **MANDATORY - WORKFLOW FAILS WITHOUT THESE:**
  - [ ] 4 Technical summaries (TradingView SPX, BTC, QQQ, SOL) ‚Üí pattern: `Research/Technicals/**/YYYY-MM-DD*Summary.md`
  - [ ] 7 Technical provider summaries (+ Fear & Greed, Market Breadth, Volatility) ‚Üí pattern: `Research/Technicals/**/YYYY-MM-DD*Summary.md`
  - [ ] 2 X summaries minimum (Crypto, Macro) ‚Üí pattern: `Research/X/YYYY-MM-DD_X_*.md`
  - [ ] X Bookmarks summary ‚Üí `Research/X/Bookmarks/YYYY-MM-DD_X_Bookmarks_Summary.md`
  - [ ] RSS provider summaries (ALL providers) ‚Üí pattern: `Research/RSS/*/YYYY-MM-DD*Summary.md`
  - [ ] YouTube channel summaries (ALL channels) ‚Üí pattern: `Research/YouTube/*/YYYY-MM-DD*Summary.md`
- **ACTION**: If ANY MANDATORY files missing ‚Üí STOP and create them. Do NOT skip, do NOT proceed without them.
- **ERROR HANDLING**: If scraper FAILS (returns error), retry ONCE. If still fails after retry, document failure and STOP workflow - report incomplete data collection
- **NO WORKAROUNDS**: There is no "skip and proceed anyway" option. All data sources are required EVERY execution.

**FIX #2: TRAD INGVIEW SUMMARY FORMAT REQUIREMENTS** (Ensure parser compatibility)
- All TradingView summaries MUST include these exact parseable formats:
  - `**Price:** [number]` or `**Price:** $[number]` (e.g., "Price: 6747.50")
  - `**50-DMA:** [number]` (e.g., "50-DMA: 6705")
  - `**200-DMA:** [number]` (e.g., "200-DMA: 6450")
- These values allow SummaryParser to extract data ‚Üí enables trend score calculation
- If values missing ‚Üí parser returns NONE ‚Üí trend score = 0 (broken)
- **VERIFICATION**: After creating/updating technical summaries, search for "Price:" + "50-DMA" + "200-DMA" in file
- **IF PARSER FAILS**: Manually review market data + apply AI adjustment per Step 4 guidance (workflow lines 498-530)

**FIX #3: MANDATORY DATA COLLECTION - NO EXCEPTIONS**
- **IF ANY SCRAPER MISSING DATA FOR TODAY:**
  - Step 1 must NOT proceed until ALL scrapers have been run
  - If Step 1 receives incomplete data: STOP and investigate scraper failure
  - Do NOT attempt to "work around" missing data by skipping providers
  - **Do NOT proceed to Step 2 without:**
    - [ ] ALL RSS provider summaries (every provider, today's data only)
    - [ ] ALL YouTube channel summaries (every channel, today's data only)
    - [ ] ALL X categories (Crypto, Macro, Technicals, Bookmarks)
    - [ ] ALL technical provider summaries (7 sources)
  - **Step 2 REQUIRES all Step 1 data** - Cannot create overviews without all provider data
  - **Step 2 outputs:**
    - Step 2.1: RSS Overview (FROM complete RSS data)
    - Step 2.2: YouTube Overview (FROM complete YouTube data)
    - Step 2.3: Technical Overview (FROM 7 technical summaries)
    - Step 2.4: X Overview (FROM complete X data)
    - Step 2.5: Key Themes (FROM all 4 overviews)

**FIX #4: SCRAPER OUTPUT HANDLING (All Sources Mandatory)**

**READ THIS FIRST:** See `Toolbox/INSTRUCTIONS/Research/SCRAPER_STATUS_GUIDE.md` for complete decision tree.

FOOLPROOF RULE: Check the status file first before deciding if something is an error.

- **STATUS FILE LOCATION**: `Research/.cache/scraper_status_YYYY-MM-DD.json`
  - Created by each scraper automatically on completion
  - Contains: `ran` (true/false), `items_found` (number), `error` (message if failed)
  - **Always check this file first** before assuming success or failure

- **SCENARIO A: EMPTY RESULTS are VALID, not failures** ‚úÖ
  - Status file shows: `"ran": true, "items_found": 0, "error": null`
  - **Meaning**: Scraper ran successfully. No new content published today.
  - **Action**: Create summary noting "Zero new [content] collected for YYYY-MM-DD"
  - **Continue workflow**: Empty is valid data; still create the summary file
  - **Examples**:
    - YouTube: `"ran": true, "items_found": 0` ‚Üí No new videos (NORMAL, proceed)
    - RSS: `"ran": true, "items_found": 0` ‚Üí No new articles (NORMAL, proceed)
    - X Bookmarks: `"ran": true, "items_found": 0` ‚Üí No new posts (NORMAL, proceed)
  - **KEY**: `items_found=0` with `ran=true` ‚â† failure. It means zero content, which is valid.

- **SCENARIO B: SCRAPER CRASHES are FAILURES** ‚ùå
  - Status file shows: `"ran": false, "error": "[Exception message]"`
  - **Meaning**: Scraper crashed or failed to complete.
  - **Action**:
    1. Read the error message in status file
    2. If transient (timeout, network): Retry scraper ONCE
    3. If persistent (permission, auth): Fix issue, then retry
    4. If still fails: STOP workflow and report incomplete data collection
  - **Do NOT skip** missing data sources. Do NOT proceed with partial data.
  - **Example**: YouTube scraper shows `"ran": false, "error": "Connection timeout"` ‚Üí Retry once. If fails again ‚Üí STOP.

- **SCENARIO C: SCRAPER MISSING FROM STATUS FILE** ‚ùå
  - Scraper name not in `Research/.cache/scraper_status_YYYY-MM-DD.json`
  - **Meaning**: Scraper never completed (crashed mid-execution or never started)
  - **Action**: STOP and investigate. Do NOT proceed with missing sources.

- **Decision Tree** (Use This):
  ```
  1. Check if status file exists for today?
     NO ‚Üí ‚ùå ERROR: No scraping ran
     YES ‚Üì
  2. Is scraper entry in status file?
     NO ‚Üí ‚ùå ERROR: Scraper didn't reach completion
     YES ‚Üì
  3. Check "ran" field
     false ‚Üí ‚ùå ERROR: Scraper failed (see error message)
     true ‚Üì
  4. Check "items_found"
     0 ‚Üí ‚úÖ OK: Zero content (NORMAL, proceed)
     >0 ‚Üí ‚úÖ OK: Found content (SUCCESS, process)
  ```

- **Templates for Zero-Content Summaries**: See `Toolbox/TEMPLATES/zero_content_summary_template.md`
  - Pre-written summaries for YouTube, X, RSS, etc.
  - Copy and customize for zero-content days
  - Ensures consistent documentation

- **Key Differences Explained**:
  | Status | items_found | Meaning | Action |
  |--------|-------------|---------|--------|
  | `ran=true` | 0 | No content published | ‚úÖ Create zero-content summary, continue |
  | `ran=true` | 5+ | Content found | ‚úÖ Process content, create summary |
  | `ran=false` | 0 | Scraper crashed | ‚ùå STOP, retry, investigate |
  | (missing) | N/A | Never completed | ‚ùå STOP, investigate |

**FIX #5: SIGNAL CALCULATION ROBUSTNESS** (Handle Parser Failures)
- If signal calculation completes but trend = 0: Likely parser extraction failed
  - Check if TradingView summaries have parseable price/MA format (Fix #2)
  - If summaries lack price/MA values ‚Üí parser returns NONE ‚Üí trend = 0
- **AI ADJUSTMENT REQUIRED** (per workflow Step 4, lines 498-530):
  - If trend = 0 but market evidence shows price near ATH + bullish technicals: Adjust trend +10 to +15
  - Document reasoning in ai_adjustments array: "Parser failed to extract data. Manual review confirms..." + evidence
  - Update composite score: new_score = original + adjustment
- **VERIFICATION**: After signal file created, verify composite >0 (if not, apply adjustment)
- **WORKFLOW CONTINUES**: Adjusted signals feed into dashboard update; don't block

**Parallel Execution Examples:**

**Example 1: Step 0 - Verify Scrapers + Run Web Searches**
```
STEP 0A: Ask user to confirm scrapers completed
  ‚Üí "Have you completed running the scrapers?"
  ‚Üí Wait for user confirmation

STEP 0B: Run 8 web searches in parallel (2-3 min)
  Send a SINGLE message with 8 WebSearch tool calls:
  - WebSearch: "SPY SPX stock market today [DATE] latest news sentiment"
  - WebSearch: "market volatility trends [DATE] investor sentiment"
  - WebSearch: "options flow SPY unusual activity [DATE]"
  - WebSearch: "institutional positioning hedge fund flows [DATE]"
  - WebSearch: "technical analysis SPY market outlook [DATE]"
  - WebSearch: "support resistance levels SPX QQQ [DATE]"
  - WebSearch: "economic data releases Fed policy [DATE]"
  - WebSearch: "earnings reports market impact [DATE]"

STEP 0C: Verify all data present
  ‚Üí Check scraper outputs exist
  ‚Üí Check web search data created
```
**Result:** User runs scrapers independently, AI handles web searches and verification

**Example 2: Step 1.3 Technical Summaries (Run 4 searches + 4 summaries in parallel)**
```
FIRST, send 4 WebSearch tool calls in parallel:
- WebSearch: "TradingView SPX technical analysis support resistance levels RSI MACD"
- WebSearch: "TradingView BTC technical analysis support resistance levels RSI MACD"
- WebSearch: "TradingView QQQ technical analysis support resistance levels RSI MACD"
- WebSearch: "TradingView SOL technical analysis support resistance levels RSI MACD"

THEN, after results return, create all 4 Write tool calls in parallel:
- Write: TradingView SPX Summary
- Write: TradingView BTC Summary
- Write: TradingView QQQ Summary
- Write: TradingView SOL Summary
```
**Result:** Web searches complete in ~1 minute (parallel) instead of 4 minutes (sequential).

**When to use parallel vs sequential:**
- **PARALLEL:** Independent tasks (different web searches, reading different files, writing different files)
- **SEQUENTIAL:** Dependent tasks (must read file BEFORE editing it; must search BEFORE writing results; must run scraper BEFORE analyzing output)

**Remember:** This workflow is research DATA COLLECTION and ANALYSIS. Do NOT:
- Update master-plan.md directly
- Modify research-dashboard.html
- Run scripts/automation/run_workflow.py (that's Master Plan's job)

================================================================================
STEP 0: GATHER MARKET DATA (MANDATORY - ALWAYS START HERE)
================================================================================

**IMPORTANT:** You MUST gather fresh market data before doing ANY processing.
Do NOT skip this step.

**‚ö° OPTIMAL EXECUTION: PARALLEL DATA COLLECTION ‚ö°**

Steps 0A and 0B run INDEPENDENTLY and can execute in PARALLEL to save time.
Step 0C synchronizes - don't proceed to Step 1 until BOTH are complete.

---

**STEP 0A: Automated Scraper Data Verification**

**üö® CRITICAL: VERIFY DATA FRESHNESS PROGRAMMATICALLY üö®**

**YOUR ACTION:**
1. **Run automated verification script:**
   ```bash
   python scripts/utilities/verify_scraper_data.py YYYY-MM-DD
   ```

2. **Check exit code:**
   - Exit 0: ‚úÖ All data fresh ‚Üí Proceed to Step 0B
   - Exit 2: ‚ùå Data incomplete/stale ‚Üí STOP WORKFLOW

3. **If exit code = 2 (data incomplete):**
   - **STOP IMMEDIATELY** - DO NOT proceed to Step 0B
   - Report to Pilot which providers are missing data
   - Ask: "Data verification failed. Retry scrapers or proceed anyway?"
   - Wait for explicit authorization before continuing

4. **If exit code = 0 (all fresh):**
   - Report: "‚úÖ Scraper data verification passed - all providers fresh"
   - Proceed automatically to Step 0B

**WHY THIS IS CRITICAL:**
- Automated verification prevents human error in data checking
- Ensures EVERY provider has today's data before analysis
- Prevents bad trading signals from stale/incomplete data
- Mission-critical safeguard: Bad data ‚Üí Bad signals ‚Üí Real financial loss

**What the User's Scraper Run Creates:**
1. ‚úÖ YouTube scraper (video transcripts)
2. ‚úÖ RSS scraper (news articles)
3. ‚úÖ X/Twitter scraper (social posts in Crypto, Macro, Technicals categories)
4. ‚úÖ X Bookmarks scraper (bookmarked posts)
5. ‚úÖ X data archival (extracts today's tweets into _archived.json files)
6. ‚úÖ Technical data scraper (SPY/QQQ options via Selenium)

**Scraper Output Mapping (Exactly What Gets Created):**

| Scraper | Source | Output Location | Format | Purpose |
|---------|--------|-----------------|--------|---------|
| YouTube | youtube_scraper.py | `Research/YouTube/{Channel}/YYYY-MM-DD*.md` | Markdown | Video transcripts |
| RSS | rss_scraper.py | `Research/RSS/{Provider}/YYYY-MM-DD*.md` | Markdown | News article summaries |
| X/Twitter Posts | x_scraper.py | `Research/X/{Category}/x_list_posts_YYYYMMDD.json` | JSON | Social posts (Crypto/Macro/Technicals) |
| X/Twitter Archived | x_scraper.py | `Research/X/{Category}/x_list_posts_YYYYMMDD_archived.json` | JSON | Today's posts archive for analysis |
| X Bookmarks | bookmarks_scraper.py | `Research/X/Bookmarks/x_bookmarks_posts_YYYYMMDD.json` | JSON | Bookmarked posts from today |
| Technical Data | fetch_technical_data.py | `Research/.cache/YYYY-MM-DD_technical_data.json` | JSON | SPY/QQQ options (max pain, P/C, IV%) |

**Output Locations (Summary):**
- YouTube: `Research/YouTube/{Channel}/YYYY-MM-DD*.md`
- RSS: `Research/RSS/{Provider}/YYYY-MM-DD*.md`
- X/Twitter Posts: `Research/X/{Category}/x_list_posts_YYYYMMDD.json`
- X/Twitter Archived: `Research/X/{Category}/x_list_posts_YYYYMMDD_archived.json` (for Step 1.4 analysis)
- X Bookmarks: `Research/X/Bookmarks/x_bookmarks_posts_YYYYMMDD.json`
- Technical Data: `Research/.cache/YYYY-MM-DD_technical_data.json`

**Data Sources:**
- YouTube API (transcripts)
- RSS feeds (various news providers)
- X/Twitter (via authenticated session in Chrome profile)
- Barchart.com (P/C ratios, IV Percentile, Volume, OI) via Selenium
- OptionCharts.io (Max Pain) via Selenium

**If you need to run scrapers individually:**

- **Technical data only:** `python scripts/processing/fetch_technical_data.py YYYY-MM-DD`
- **YouTube only:** Run `Scraper/youtube_scraper.py`
- **RSS only:** Run `Scraper/rss_scraper.py`
- **X/Twitter only:** Run `Scraper/x_scraper.py`
- **X Bookmarks only:** Run `Scraper/bookmarks_scraper.py`

---

**STEP 0B: WEB SEARCH FOR MARKET DATA (After User Confirms Scrapers Complete)**

**‚ö° EXECUTE AFTER USER CONFIRMATION** - Run after user confirms scrapers finished.

Use the `WebSearch` tool to gather real-time market data from the following areas.

**Duration:** 2-3 minutes (when run in parallel)
**Dependencies:** User must confirm Step 0A scraper completion first

**Required Searches:**

1. **Market Performance & Sentiment**
   - Query: "SPY SPX stock market today [DATE] latest news sentiment"
   - Query: "market volatility trends [DATE] investor sentiment"

2. **Options Flow & Positioning**
   - Query: "options flow SPY unusual activity [DATE]"
   - Query: "institutional positioning hedge fund flows [DATE]"

3. **Technical Analysis**
   - Query: "technical analysis SPY market outlook [DATE]"
   - Query: "support resistance levels SPX QQQ [DATE]"

4. **Economic & Macro**
   - Query: "economic data releases Fed policy [DATE]"
   - Query: "earnings reports market impact [DATE]"

**Output Location:**
- Save consolidated findings to: `Research/.cache/YYYY-MM-DD_market_data.md`
  (**Standard Format:** Use `.md` for consistency with other research outputs. Use `.json` ONLY if you need structured data for automated processing.)

---

**STEP 0C: VERIFY ALL DATA IS PRESENT (Synchronization Point)**

**‚è≥ CRITICAL GATE: DO NOT PROCEED TO STEP 1 UNTIL VERIFICATION COMPLETE ‚è≥**

This is the verification point. Both Step 0A (user-run scrapers) and Step 0B (web searches) must have valid outputs before proceeding.

**Verification Checklist (BOTH Step 0A AND 0B):**

**Step 0A Outputs (User-Run Scrapers):**
- [ ] User confirmed scraper completion
- [ ] RSS files exist for today (Research/RSS/*/YYYY-MM-DD*.md) - use Glob to verify
- [ ] YouTube files exist for today (Research/YouTube/*/YYYY-MM-DD*.md) - use Glob to verify
- [ ] Technical summaries exist for today (Research/Technicals/*/YYYY-MM-DD*_Summary.md) - use Glob to verify
- [ ] X data files exist for today (Research/X/*/x_list_posts_YYYYMMDD*.json) - use Glob to verify
- [ ] X archived files exist (Research/X/*/x_list_posts_YYYYMMDD_archived.json) - use Glob to verify
- [ ] Technical data JSON exists (Research/.cache/YYYY-MM-DD_technical_data.json) - use Read to verify

**Step 0B Outputs:**
- [ ] Web searches completed (all 8 searches returned results)
- [ ] Market data file created (Research/.cache/YYYY-MM-DD_market_data.md)
- [ ] File contains: SPY/SPX performance, volatility metrics, sentiment indicators, technical levels, options flow, catalysts

**ONLY AFTER ALL CHECKBOXES VERIFIED** ‚Üí Proceed to Step 1

**Expected Total Time for Step 0:**
- Parallel execution: ~10-20 minutes (limited by slower Step 0A scrapers)
- Sequential execution: ~13-23 minutes (10-20 min scrapers + 3 min web searches)
- **Time saved by parallel approach: ~3 minutes**

**Error Handling & Recovery Procedures:**

| Failure Scenario | Retry Action | If Still Fails | Notes |
|-----------------|--------------|-----------------|-------|
| WebSearch times out | Retry ONCE with same query | Proceed with available data | Max 2 attempts per query |
| WebSearch returns empty results | Try alternative query phrasing (e.g., "market sentiment today") | Use domain-specific search (marketwatch.com, yahoo finance) | Try 3 alternative queries |
| All WebSearch fails | Skip web search component | Create market_data.md from scraped data only | Note missing data in checklist |
| Scraper crashes (YouTube/RSS) | Run individual scraper again | Proceed with available providers | Document which scrapers failed |
| Technical scraper fails (Selenium) | Check if website down, retry once | Proceed without options data | Mark technical_data.json as incomplete |
| JSON parsing errors | Validate JSON format | Create markdown summary instead | Use `jq` tool to validate |
| No data files created after Step 0 | Verify paths exist, check permissions | Create placeholder files with "No Data" notes | Do NOT stop workflow |

**When recovering from failures:**
1. Log the failure (what failed, when, what data was lost)
2. Try ONE retry (not unlimited retries)
3. If retry fails, proceed with available data
4. Document gaps in final completion report
5. **ALWAYS continue workflow** - do not let one scraper failure block the entire process

**üìä STEP 0 COMPLETE** ‚Üí Proceed to STEP 1 below

================================================================================
STEP 1: INDIVIDUAL PROVIDER SUMMARIES
================================================================================

**Goal:** Create a single summary file for each individual data provider (e.g., a summary for MarketWatch, a summary for CNBC, a summary for the 42 Macro YouTube channel, etc.).

**Process:**

### 1.1: RSS Providers

1.  **List Providers:** Use `Glob` with `Research/RSS/*/` to see all provider folders.
2.  **For each provider folder (e.g., MarketWatch, CNBC):**
    a. **List Articles:** Use `Glob` to find matching files (e.g., `Research/RSS/MarketWatch/YYYY-MM-DD*.md`) to find all raw article files for today's date.
    b. **Read Articles:** Use `Read` tool on each file to read the content.
    c. **Summarize & Save:** Synthesize the content into a single summary. Save it inside the *same provider folder* as `YYYY-MM-DD_<ProviderName>_Summary.md` using the `Write` tool.

**NOTE ON RSS ISSUES:** If articles only contain headlines, summarize the headlines as per the original instructions.

### 1.2: YouTube Providers

1.  **List Channels:** Use `Glob` with `Research/YouTube/*/` to see all channel folders.
2.  **For each channel folder:**
    a. **List Transcripts:** Use `Glob` with `Research/YouTube/<ChannelName>/YYYY-MM-DD*.md` to find any new transcript files for today's date.
    b. **Read & Summarize:** If new transcripts exist, use `Read` tool to read them, summarize the content, and save it inside the *same channel folder* as `YYYY-MM-DD_<ChannelName>_Summary.md` using the `Write` tool.
    c. **Handle Failures:** If the scraper failed for a channel (as noted in the Step 0 output), skip it and note the failure.

**Verification Step:** Before creating a "No Data Available" summary for the entire category, be certain that no files exist. Use `Glob` to check each individual provider folder to get a definitive list of its contents. If, after checking every folder, no files for the current date are found, then and only then should you create a placeholder summary noting the failure.

### 1.3: Technical Providers

**‚ö†Ô∏è CRITICAL: ALWAYS CREATE THESE 4 MINIMUM TECHNICAL SUMMARIES:**

Before proceeding to other steps, you MUST create technical summaries for these 4 core assets:
1. **TradingView SPX** - S&P 500 equity index
2. **TradingView BTC** - Bitcoin crypto benchmark
3. **TradingView QQQ** - Nasdaq 100 tech index
4. **TradingView SOL** - Solana crypto altcoin

**IMPORTANT NOTE on Step 0B vs Step 1.3 Web Searches:**
- **Step 0B:** General market data (SPY/SPX performance, volatility, options flow, Fed policy, earnings)
- **Step 1.3:** Specific technical analysis (detailed support/resistance, RSI/MACD readings, chart patterns for 4 core assets)
- **These are SEPARATE searches.** Step 1.3 focuses on deep technical analysis, not general market sentiment.

**Process:**

1. **For EACH of the 4 core assets:**
   - Run web search using `WebSearch` tool with query: `TradingView [Asset] technical analysis support resistance levels RSI MACD`
   - Extract key metrics: support/resistance levels, RSI reading, MACD status, moving averages, chart patterns, trend assessment
   - Create summary file using `Write` tool: `Research/Technicals/<ProviderName>/YYYY-MM-DD_<ProviderName>_Summary.md`
   - **Include format:** Current price/level, key support/resistance, technical indicators, trend status, trading implications

   **OPTIMIZATION:** Run all 4 web searches in PARALLEL using a single message with 4 WebSearch tool calls to save time.

2. **REQUIRED additional technical provider summaries:**

   After creating the 4 core assets, you MUST create summaries for these additional providers:
   - **Fear & Greed Index** (market fear/greed sentiment)
   - **Market Breadth** (advance/decline ratios, market participation)
   - **Volatility Metrics** (VIX levels, IV percentiles, trend)

   Process for each: Run WebSearch, extract key data, create summary in `Research/Technicals/{ProviderName}/YYYY-MM-DD_{ProviderName}_Summary.md`

3. **Reference previous examples:**
   - See `Research/Technicals/TradingView SPX/` for format examples
   - Each summary should be 400-600 words synthesizing analysis

4. **Optional additional providers (if time permits):**
   - CoinGlass (crypto positioning)
   - FRED (economic data)
   - On-Chain metrics (blockchain activity)
   - Other specialized indicators
   - These enhance analysis but are NOT blocking for workflow

**Verification Checklist (MANDATORY):**

Before proceeding to Step 1.4, verify ALL required technical summaries exist using `Glob`:

**CORE ASSETS (4 REQUIRED):**
```
Research/Technicals/TradingView SPX/YYYY-MM-DD_TradingView SPX_Summary.md
Research/Technicals/TradingView BTC/YYYY-MM-DD_TradingView BTC_Summary.md
Research/Technicals/TradingView QQQ/YYYY-MM-DD_TradingView QQQ_Summary.md
Research/Technicals/TradingView SOL/YYYY-MM-DD_TradingView SOL_Summary.md
```

**ADDITIONAL REQUIRED PROVIDERS (3 REQUIRED):**
```
Research/Technicals/Fear & Greed Index/YYYY-MM-DD_Fear & Greed Index_Summary.md
Research/Technicals/Market Breadth/YYYY-MM-DD_Market Breadth_Summary.md
Research/Technicals/Volatility Metrics/YYYY-MM-DD_Volatility Metrics_Summary.md
```

**TOTAL: 7 required technical summaries before Step 1.4**

Use Glob pattern: `Research/Technicals/**/YYYY-MM-DD*Summary.md` to verify all 7 exist.

**If ANY of these 7 files are missing:**
- STOP and create them before proceeding
- The signal calculation workflow (Steps 4-8) will FAIL without these files
- These 7 summaries are prerequisites, not optional enhancements

### 1.4: X (Twitter) Social Sentiment

**‚ö†Ô∏è CRITICAL: USE TASK TOOL FOR X DATA ANALYSIS - DO NOT READ DIRECTLY ‚ö†Ô∏è**

X data files are MASSIVE (500-800+ posts per file, 80,000+ tokens). Direct reading with `Read` tool will fail due to token limits.

**CORRECT APPROACH (MANDATORY):**

1. **Use Task tool with general-purpose agent** to analyze ALL X data:

```
Task(
  subagent_type="general-purpose",
  description="Analyze X/Twitter posts comprehensively",
  prompt="Create comprehensive X/Twitter summaries by reading ALL posts from these files:

  1. Research/X/Crypto/x_list_posts_YYYYMMDD_archived.json (typically 400-500 posts)
  2. Research/X/Technicals/x_list_posts_YYYYMMDD_archived.json (typically 90-100 posts)
  3. Research/X/Macro/x_list_posts_YYYYMMDD*.json (use latest file, typically 200+ posts)
  4. Research/X/Bookmarks/x_list_posts_YYYYMMDD*.json (typically 1-10 posts)

  For EACH category, you MUST:
  1. Read the COMPLETE JSON file (all posts, not a sample)
  2. Analyze sentiment (bullish/bearish/neutral counts)
  3. Identify most discussed tickers (with mention counts)
  4. Extract key themes and narratives
  5. Note high-engagement posts
  6. Create a comprehensive markdown summary

  Save each summary to: Research/X/YYYY-MM-DD_X_[Category]_Summary.md

  This is MANDATORY - read ALL posts, do not sample."
)
```

2. **Agent will create 4 summary files:**
   - `Research/X/YYYY-MM-DD_X_Crypto_Summary.md`
   - `Research/X/YYYY-MM-DD_X_Technicals_Summary.md`
   - `Research/X/YYYY-MM-DD_X_Macro_Summary.md`
   - `Research/X/YYYY-MM-DD_X_Bookmarks_Summary.md`

3. **Required Format for Each Summary:**
```markdown
# X/Twitter [Category] Summary - [Date]

**Total Posts Analyzed:** [number]

## Sentiment Analysis
- Bullish: X posts (X%)
- Bearish: X posts (X%)
- Neutral: X posts (X%)
- **Sentiment Score:** X/100

## Most Discussed Tickers
[List top 10+ tickers with mention counts]

## Key Themes
[List dominant themes with descriptions]

## Notable Posts
[5-10 high-impact posts with context]

## Summary
[2-3 paragraph narrative]
```

4. **Verification:** Use `Glob` to confirm all 4 summary files exist before proceeding to Step 2

**DO NOT:**
- ‚ùå Try to read JSON files directly with Read tool
- ‚ùå Read in chunks sequentially (too slow, incomplete)
- ‚ùå Sample posts (must analyze ALL posts)
- ‚ùå Create summaries without using Task tool

**WHY THIS MATTERS:**
- X data is typically 700-800 total posts across 4 files
- Files can be 80,000+ tokens (exceeds Read tool limits)
- Task tool agent can handle full dataset efficiently
- Comprehensive analysis is MANDATORY per user requirements

================================================================================
STEP 2: CATEGORY OVERVIEWS (ROLL-UP)
================================================================================

**Goal:** Create a single overview file for each main data category (RSS, YouTube, Technical, X).

**‚ö° OPTIMIZATION: USE TASK TOOL IN PARALLEL FOR ALL 4 CATEGORY OVERVIEWS ‚ö°**

Category overviews consolidate multiple provider summaries. Running them in parallel saves significant time (~8-10 minutes).

**RECOMMENDED APPROACH:**

Send a **single message** with **4 Task tool calls** (one for each category overview):

### 2.1: Create RSS Category Overview (Parallel Task 1)

```
Task(
  subagent_type="general-purpose",
  description="Create RSS category overview",
  prompt="Create an RSS Category Overview by consolidating all RSS provider summaries.

  Read these files:
  - Research/RSS/MarketWatch/YYYY-MM-DD_MarketWatch_Summary.md
  - Research/RSS/CNBC/YYYY-MM-DD_CNBC_Summary.md
  - Research/RSS/Seeking Alpha/YYYY-MM-DD_Seeking Alpha_Summary.md
  - Research/RSS/CoinDesk/YYYY-MM-DD_CoinDesk_Summary.md
  - Research/RSS/Federal Reserve/YYYY-MM-DD_Federal Reserve_Summary.md

  Consolidate: sentiment, common themes, unique insights, conflicts
  Save to: Research/YYYY-MM-DD_RSS_Category_Overview.md"
)
```

### 2.2: Create YouTube Category Overview (Parallel Task 2)

```
Task(
  subagent_type="general-purpose",
  description="Create YouTube category overview",
  prompt="Create YouTube Category Overview by analyzing all YouTube video transcripts.

  Read all files: Research/YouTube/*/YYYY-MM-DD*.md

  Synthesize: key messages, common themes, diverging opinions, sentiment
  Save to: Research/YYYY-MM-DD_YouTube_Category_Overview.md"
)
```

### 2.3: Create Technical Category Overview (Parallel Task 3)

```
Task(
  subagent_type="general-purpose",
  description="Create Technical category overview",
  prompt="Create Technical Category Overview by consolidating all 7 technical summaries.

  Read files:
  - Research/Technicals/TradingView SPX/YYYY-MM-DD_TradingView SPX_Summary.md
  - Research/Technicals/TradingView BTC/YYYY-MM-DD_TradingView BTC_Summary.md
  - Research/Technicals/TradingView QQQ/YYYY-MM-DD_TradingView QQQ_Summary.md
  - Research/Technicals/TradingView SOL/YYYY-MM-DD_TradingView SOL_Summary.md
  - Research/Technicals/Fear & Greed Index/YYYY-MM-DD_Fear & Greed Index_Summary.md
  - Research/Technicals/Market Breadth/YYYY-MM-DD_Market Breadth_Summary.md
  - Research/Technicals/Volatility Metrics/YYYY-MM-DD_Volatility Metrics_Summary.md

  Consolidate: technical signals, support/resistance, market health, warnings
  Save to: Research/YYYY-MM-DD_Technical_Category_Overview.md"
)
```

### 2.4: Create X/Twitter Category Overview (Parallel Task 4)

```
Task(
  subagent_type="general-purpose",
  description="Create X/Twitter category overview",
  prompt="Create X/Twitter Category Overview by consolidating all 4 X summaries.

  Read files:
  - Research/X/YYYY-MM-DD_X_Crypto_Summary.md
  - Research/X/YYYY-MM-DD_X_Technicals_Summary.md
  - Research/X/YYYY-MM-DD_X_Macro_Summary.md
  - Research/X/YYYY-MM-DD_X_Bookmarks_Summary.md

  Consolidate: sentiment by category, cross-category tickers, key themes, divergences
  Save to: Research/YYYY-MM-DD_X_Category_Overview.md"
)
```

**RESULT:** All 4 category overviews created simultaneously in ~2-3 minutes (vs ~8-10 minutes sequential)

### 2.5: Create Key Themes Document

1.  **Read ALL Category Overviews:** Use `Glob` or `Read` tool to load:
    - `Research/RSS/YYYY-MM-DD_RSS_Overview.md` (from Step 2.1)
    - `Research/YouTube/YYYY-MM-DD_YouTube_Overview.md` (from Step 2.2)
    - `Research/Technicals/YYYY-MM-DD_Technical_Overview.md` (from Step 2.3)
    - `Research/X/YYYY-MM-DD_X_Overview.md` (from Step 2.4)

2.  **Identify Cross-Provider Themes:**
    - What themes appear in 3+ sources? (High consensus)
    - What are the dominant narratives (bullish vs bearish)?
    - What catalysts are everyone watching?
    - What key levels/events are mentioned repeatedly?
    - What disagreements exist between sources?

3.  **Structure & Save:** Create a prioritized list of themes by importance:
    - Severity/Impact rating (CRITICAL, HIGH, MEDIUM, LOW)
    - Sources mentioning it (which providers agree?)
    - Timeline (near-term vs medium-term)
    - Trading implications

    Save as: `Research/.cache/YYYY-MM-DD_key_themes.md`

**Reference Example:** See `Research/.cache/2025-10-17_key_themes.md` for format

================================================================================
STEP 3: FINAL MARKET SENTIMENT OVERVIEW
================================================================================

**Goal:** Create the final, top-level market overview for the day.

**Sources to review:**
- `Research/RSS/YYYY-MM-DD_RSS_Overview.md` (from Step 2.1)
- `Research/YouTube/YYYY-MM-DD_YouTube_Overview.md` (from Step 2.2)
- `Research/Technicals/YYYY-MM-DD_Technical_Overview.md` (from Step 2.3)
- `Research/X/YYYY-MM-DD_X_Overview.md` (from Step 2.4)
- `Research/.cache/YYYY-MM-DD_key_themes.md` (from Step 2.5)

**File to create:**
`Research/.cache/YYYY-MM-DD_Market_Sentiment_Overview.md`

**Note:** Saved to `.cache` folder (not root) for consistency with other daily artifacts

**Overview should include:**
1.  **Executive Summary** (2-3 sentences: overall market tone)
2.  **Cross-Provider Consensus** (common themes across sources)
3.  **Key Levels to Watch** (SPX, BTC, QQQ, major levels)
4.  **Major Catalysts** (events, data, earnings this week)
5.  **Aggregate Sentiment Score** (0-100 qualitative assessment)
6.  **Risk Level** (LOW/MEDIUM/HIGH based on volatility and positioning)

**Format:**
```markdown
# Market Sentiment Overview - [Date]

## Executive Summary
[2-3 sentences]

## Cross-Provider Consensus
### Bulls Say:
- [Common bullish themes]

### Bears Say:
- [Common bearish themes]

## Key Levels to Watch
- SPX: [levels]
- BTC: [levels]
- QQQ: [levels]

## Major Catalysts This Week
- [Events, data, earnings]

## Sentiment Score: [0-100]
[Brief justification]

## Risk Level: [LOW/MEDIUM/HIGH]
[Brief justification]
```

================================================================================
STEP 4: CALCULATE TRADING SIGNALS
================================================================================

**Goal:** Transform the research data into quantitative trading signals.

**What This Does:**
The signal calculator analyzes all your research summaries and market data to produce a composite signal score (0-100) that indicates the strength of the current dip-buying opportunity.

**Process:**

1. **Run Signal Calculator**

   Execute the signal calculation script:
   ```bash
   python scripts/processing/calculate_signals.py YYYY-MM-DD
   ```

2. **What Gets Analyzed:**

   The calculator automatically reads:
   - Technical summaries (TradingView SPX, BTC, QQQ, SOL)
   - Market breadth data
   - Volatility metrics (VIX, IV percentiles)
   - X sentiment summaries (Crypto, Macro)
   - Market data from APIs (if available)

3. **Signal Components:**

   Five weighted components create the composite score:
   - **Trend (40%)**: Position relative to moving averages, dip-buy scoring
   - **Breadth (25%)**: Market participation, contrarian-adjusted by X sentiment
   - **Volatility (20%)**: VIX levels, implied volatility (higher = better dip-buy)
   - **Technical (10%)**: RSI oversold conditions
   - **Seasonality (5%)**: Historical monthly patterns

4. **Output:**

   Creates: `Research/.cache/signals_YYYY-MM-DD.json`

   Contains:
   - Composite score (0-100)
   - Tier classification (WEAK/MODERATE/STRONG/EXTREME)
   - Component breakdown with explanations
   - X sentiment scores with contrarian adjustments

5. **Review Signals:**

   After calculation completes, review the output:
   - Use `Read` tool to load: `Research/.cache/signals_YYYY-MM-DD.json`
   - Review composite score, tier, and component breakdown
   - Verify accuracy against your research data

6. **Optional AI Adjustments:**

   **When to Make Adjustments:** Only adjust if the signal score contradicts clear market evidence from your research data.

   **Adjustment Criteria (Decision Tree):**

   | Component | When to Adjust | Example Reasoning | Typical Range |
   |-----------|---------------|--------------------|---------------|
   | **Trend** | Price held support but calc=0 | Buyers defending level despite weak stats | +5 to +15 |
   | **Breadth** | Multiple sources bullish, calc low | Data sources limited by availability | +5 to +10 |
   | **Volatility** | VIX spiked higher, calc outdated | Real-time volatility > data vintage | +3 to +8 |
   | **Technical** | Multiple indicators agree, calc=0 | Oversold RSI + support holding | +5 to +12 |
   | **Seasonality** | Major catalyst this week | Fed announcement, earnings, data release | +2 to +8 |

   **How to Document Adjustments:**
   - Edit `Research/.cache/signals_YYYY-MM-DD.json` directly
   - Add entries to the "ai_adjustments" array:
     ```json
     "ai_adjustments": [
       {
         "component": "Trend",
         "original_score": 12.5,
         "adjusted_score": 24.0,
         "reasoning": "Support held at 5650 with 3 attempts. Buyers clearly present. Script may have missed recent price action."
       }
     ]
     ```
   - Updated composite = original + sum of adjustments
   - **Max total adjustments: ¬±15 points** (prevents over-correction)

   **RED FLAGS - Do NOT Adjust For:**
   - Personal prediction about tomorrow's market (stick to data evidence)
   - Feeling that "it should be higher" without data backing (provide research citations)
   - Adjusting because signal doesn't match your bias (that's the whole point - objectivity)

**Verification:**

Before proceeding to Step 5, confirm:
- [ ] Signal calculation completed successfully
- [ ] Composite score is between 0-100
- [ ] All component scores are present
- [ ] Tier classification assigned (WEAK/MODERATE/STRONG/EXTREME)
- [ ] X sentiment scores extracted
- [ ] JSON file is valid (no syntax errors)

**Example Output Summary:**
```
Composite Score: 72.5/100
Tier: STRONG
Breakdown:
  Trend: 32.00/40
  Breadth: 19.50/25
  Volatility: 14.00/20
  Technical: 5.00/10
  Seasonality: 2.00/5
```

================================================================================
STEP 5: FINALIZE RESEARCH WORKFLOW
================================================================================

**Goal:** Complete the research workflow and prepare handoff to dashboard.

**üìã Final Completion Checklist:**

Before reporting completion, verify ALL steps completed:

**Step 0: Data Collection**
- [ ] Automated technical data scraper executed (fetch_technical_data.py)
- [ ] Technical data saved to Research/.cache/YYYY-MM-DD_technical_data.json
- [ ] SPY options data verified (max pain, P/C ratios, IV percentile)
- [ ] QQQ options data verified (max pain, P/C ratios, IV percentile)
- [ ] Web searches executed (market performance, volatility, options flow, technicals)
- [ ] Market data saved to Research/.cache/YYYY-MM-DD_market_data.md (standard format)

**Step 1: Provider Summaries**
- [ ] Technical summaries created (TradingView SPX, BTC, QQQ, SOL - REQUIRED)
- [ ] Additional technical providers (Fear & Greed, Market Breadth, etc. - optional)
- [ ] X sentiment summaries (if applicable)

**Step 2: Category Overviews**
- [ ] RSS Overview created (Step 2.1)
- [ ] YouTube Overview created (Step 2.2)
- [ ] Technical Overview created (Step 2.3)
- [ ] X/Twitter Overview created (Step 2.4)
- [ ] Key themes identified and documented (Step 2.5)
- [ ] Research/.cache/YYYY-MM-DD_key_themes.md created

**Step 3: Market Sentiment Overview**
- [ ] Final market overview created: Research/.cache/YYYY-MM-DD_Market_Sentiment_Overview.md
- [ ] Executive summary, cross-provider consensus, key levels, catalysts included
- [ ] Sentiment score and risk level assigned

**Step 4: Signal Calculation**
- [ ] Trading signals calculated: Research/.cache/signals_YYYY-MM-DD.json
- [ ] Composite score, tier, and component breakdown present
- [ ] AI review completed (adjustments made if needed)

**üìä Output Files to Verify:**

Use Glob to verify all created files exist (pattern matching):
```
Research/**/YYYY-MM-DD*.md
Research/**/YYYY-MM-DD*.json
```

Minimum required files (16 total):

**Data Collection:**
1. `Research/.cache/YYYY-MM-DD_market_data.md` (from Step 0B web searches - standard format)
2. `Research/.cache/YYYY-MM-DD_technical_data.json` (from Step 0A automated scraper)

**Provider Summaries (Step 1):**
3. `Research/Technicals/TradingView SPX/YYYY-MM-DD_TradingView SPX_Summary.md`
4. `Research/Technicals/TradingView BTC/YYYY-MM-DD_TradingView BTC_Summary.md`
5. `Research/Technicals/TradingView QQQ/YYYY-MM-DD_TradingView QQQ_Summary.md`
6. `Research/Technicals/TradingView SOL/YYYY-MM-DD_TradingView SOL_Summary.md`
7. `Research/X/YYYY-MM-DD_X_Crypto_Summary.md` (from Step 1.4)
8. `Research/X/YYYY-MM-DD_X_Macro_Summary.md` (from Step 1.4)
9. `Research/X/Bookmarks/YYYY-MM-DD_X_Bookmarks_Summary.md` (from Step 1.5)

**Category Overviews (Step 2):**
10. `Research/RSS/YYYY-MM-DD_RSS_Overview.md` (Step 2.1)
11. `Research/YouTube/YYYY-MM-DD_YouTube_Overview.md` (Step 2.2)
12. `Research/Technicals/YYYY-MM-DD_Technical_Overview.md` (Step 2.3)
13. `Research/X/YYYY-MM-DD_X_Overview.md` (Step 2.4)
14. `Research/.cache/YYYY-MM-DD_key_themes.md` (Step 2.5)

**Final Output (Step 3-4):**
15. `Research/.cache/YYYY-MM-DD_Market_Sentiment_Overview.md` (Step 3)
16. `Research/.cache/signals_YYYY-MM-DD.json` (Step 4)

**üìû Report Completion:**

When all steps complete, report with this structure:

```
üéØ RESEARCH WORKFLOW COMPLETE FOR [DATE]

‚úÖ STEP 0: Data Collection
  - Automated technical data collected (or noted if unavailable)
  - Market data gathered via web search

‚úÖ STEP 1: Provider Summaries (4 REQUIRED + Optional)
  - TradingView SPX Summary
  - TradingView BTC Summary
  - TradingView QQQ Summary
  - TradingView SOL Summary
  - X Crypto Sentiment Summary
  - X Macro Sentiment Summary
  - X Bookmarks Summary

‚úÖ STEP 2: Category Overviews
  - RSS Overview
  - YouTube Overview
  - Technical Overview
  - X/Twitter Overview
  - Key Themes identified

‚úÖ STEP 3: Market Sentiment Overview
  - Executive summary created
  - Sentiment score assigned (0-100)
  - Risk level assessed

‚úÖ STEP 4: Trading Signals
  - Composite score calculated: [XX/100]
  - Tier assigned: [WEAK/MODERATE/STRONG/EXTREME]
  - AI adjustments applied (if any): [List]

üìä Output Files Created (16 minimum):
- Research/.cache/YYYY-MM-DD_Market_Sentiment_Overview.md
- Research/.cache/YYYY-MM-DD_market_data.md (standard markdown format)
- Research/.cache/YYYY-MM-DD_technical_data.json
- Research/.cache/YYYY-MM-DD_key_themes.md
- Research/.cache/signals_YYYY-MM-DD.json
- Research/Technicals/TradingView SPX/YYYY-MM-DD_TradingView SPX_Summary.md
- Research/Technicals/TradingView BTC/YYYY-MM-DD_TradingView BTC_Summary.md
- Research/Technicals/TradingView QQQ/YYYY-MM-DD_TradingView QQQ_Summary.md
- Research/Technicals/TradingView SOL/YYYY-MM-DD_TradingView SOL_Summary.md
- Research/X/YYYY-MM-DD_X_Crypto_Summary.md
- Research/X/YYYY-MM-DD_X_Macro_Summary.md
- Research/X/Bookmarks/YYYY-MM-DD_X_Bookmarks_Summary.md
- Research/RSS/YYYY-MM-DD_RSS_Overview.md
- Research/YouTube/YYYY-MM-DD_YouTube_Overview.md
- Research/X/YYYY-MM-DD_X_Overview.md
- Research/Technicals/YYYY-MM-DD_Technical_Overview.md

‚ö†Ô∏è  Issues Encountered: [None or describe issues and workarounds]

üöÄ Status: ‚úÖ WORKFLOW COMPLETE - READY FOR MASTER PLAN HANDOFF
```

**üéØ Research workflow complete!**

The dashboard update workflow (run_workflow.py) will now:
- Read your research summaries and signals
- Update master-plan.md with latest data
- Update research-dashboard.html for visualization
- Verify consistency across all systems

================================================================================
‚ö†Ô∏è  RESEARCH WORKFLOW ENDS HERE - DO NOT UPDATE DASHBOARD ‚ö†Ô∏è
================================================================================

**CRITICAL: The Research workflow does NOT update the HTML dashboard.**

This workflow is ONLY responsible for:
- Data collection (scrapers, web searches)
- Creating markdown summaries and analysis files
- Calculating trading signals (signals_YYYY-MM-DD.json)

**DO NOT run any of these scripts as part of the Research workflow:**
- ‚ùå scripts/automation/run_workflow.py
- ‚ùå scripts/automation/update_master_plan.py
- ‚ùå Any script that touches master-plan/research-dashboard.html

**HANDOFF TO MASTER PLAN WORKFLOW:**
After completing Steps 0-5 above, your work is done. Report completion to the user and inform them that the next step is to execute the Master Plan workflow:

**Command to hand off:**
"Research workflow complete. Please execute: @master-plan/How to use_MP_CLAUDE_ONLY.txt"

The Master Plan workflow will handle:
- Running scripts/automation/run_workflow.py
- Updating master-plan.md with your research data
- Updating research-dashboard.html for visualization
- Verifying consistency across all systems
